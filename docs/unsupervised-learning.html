<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Topic 4 Unsupervised Learning | A Tour of Machine Learning Tools</title>
  <meta name="description" content="A tour of selected machine learning tools that are used in plant pathogenomics research" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Topic 4 Unsupervised Learning | A Tour of Machine Learning Tools" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A tour of selected machine learning tools that are used in plant pathogenomics research" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Topic 4 Unsupervised Learning | A Tour of Machine Learning Tools" />
  
  <meta name="twitter:description" content="A tour of selected machine learning tools that are used in plant pathogenomics research" />
  

<meta name="author" content="Dan MacLean" />


<meta name="date" content="2021-04-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="r-fundamentals.html"/>
<link rel="next" href="supervised-learning.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro to ML</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Setting up</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#knowledge-prerequisites"><i class="fa fa-check"></i><b>1.1.1</b> Knowledge prerequisites</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#software-prerequisites"><i class="fa fa-check"></i><b>1.1.2</b> Software prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#installing-r"><i class="fa fa-check"></i><b>1.2</b> Installing R</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#installing-rstudio"><i class="fa fa-check"></i><b>1.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#installing-r-packages-in-rstudio"><i class="fa fa-check"></i><b>1.4</b> Installing R packages in RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#standard-packages"><i class="fa fa-check"></i><b>1.4.1</b> Standard packages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="motivation.html"><a href="motivation.html"><i class="fa fa-check"></i><b>2</b> Motivation</a><ul>
<li class="chapter" data-level="2.1" data-path="motivation.html"><a href="motivation.html#open-the-pod-bay-doors-please-hal."><i class="fa fa-check"></i><b>2.1</b> Open the pod bay doors, please, HAL.</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="r-fundamentals.html"><a href="r-fundamentals.html"><i class="fa fa-check"></i><b>3</b> R Fundamentals</a><ul>
<li class="chapter" data-level="3.1" data-path="r-fundamentals.html"><a href="r-fundamentals.html#about-this-chapter"><i class="fa fa-check"></i><b>3.1</b> About this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="r-fundamentals.html"><a href="r-fundamentals.html#working-with-r"><i class="fa fa-check"></i><b>3.2</b> Working with R</a></li>
<li class="chapter" data-level="3.3" data-path="r-fundamentals.html"><a href="r-fundamentals.html#variables"><i class="fa fa-check"></i><b>3.3</b> Variables</a><ul>
<li class="chapter" data-level="3.3.1" data-path="r-fundamentals.html"><a href="r-fundamentals.html#using-objects-and-functions"><i class="fa fa-check"></i><b>3.3.1</b> Using objects and functions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="r-fundamentals.html"><a href="r-fundamentals.html#dataframes"><i class="fa fa-check"></i><b>3.4</b> Dataframes</a></li>
<li class="chapter" data-level="3.5" data-path="r-fundamentals.html"><a href="r-fundamentals.html#packages"><i class="fa fa-check"></i><b>3.5</b> Packages</a></li>
<li class="chapter" data-level="3.6" data-path="r-fundamentals.html"><a href="r-fundamentals.html#using-r-help"><i class="fa fa-check"></i><b>3.6</b> Using R Help</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>4</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#about-this-chapter-1"><i class="fa fa-check"></i><b>4.1</b> About this chapter</a></li>
<li class="chapter" data-level="4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#p-features-and-n-cases"><i class="fa fa-check"></i><b>4.2</b> <span class="math inline">\(p\)</span> Features and <span class="math inline">\(n\)</span> Cases</a></li>
<li class="chapter" data-level="4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering"><i class="fa fa-check"></i><b>4.3</b> Clustering</a><ul>
<li class="chapter" data-level="4.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#the-distance-measure-and-matrix"><i class="fa fa-check"></i><b>4.3.1</b> The distance measure and matrix</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-single-linkage-clustering"><i class="fa fa-check"></i><b>4.4</b> Hierarchical (single linkage) Clustering</a><ul>
<li class="chapter" data-level="4.4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#hierarchical-clustering-in-base-r"><i class="fa fa-check"></i><b>4.4.1</b> Hierarchical clustering in Base R</a></li>
<li class="chapter" data-level="4.4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustered-heatmaps"><i class="fa fa-check"></i><b>4.4.2</b> Clustered Heatmaps</a></li>
<li class="chapter" data-level="4.4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#extra-credit-ggplot-and-clusters"><i class="fa fa-check"></i><b>4.4.3</b> Extra Credit: ggplot and clusters</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>4.5</b> K-Means clustering</a><ul>
<li class="chapter" data-level="4.5.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#figure-of-merit"><i class="fa fa-check"></i><b>4.5.1</b> Figure of Merit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>5</b> Supervised Learning</a><ul>
<li class="chapter" data-level="5.1" data-path="supervised-learning.html"><a href="supervised-learning.html#about-this-chapter-2"><i class="fa fa-check"></i><b>5.1</b> About this chapter</a></li>
<li class="chapter" data-level="5.2" data-path="supervised-learning.html"><a href="supervised-learning.html#labelled-data"><i class="fa fa-check"></i><b>5.2</b> Labelled Data</a></li>
<li class="chapter" data-level="5.3" data-path="supervised-learning.html"><a href="supervised-learning.html#training-and-testing"><i class="fa fa-check"></i><b>5.3</b> Training and Testing</a><ul>
<li class="chapter" data-level="5.3.1" data-path="supervised-learning.html"><a href="supervised-learning.html#the-training-phase"><i class="fa fa-check"></i><b>5.3.1</b> The Training Phase</a></li>
<li class="chapter" data-level="5.3.2" data-path="supervised-learning.html"><a href="supervised-learning.html#the-testing-phase"><i class="fa fa-check"></i><b>5.3.2</b> The Testing Phase</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="supervised-learning.html"><a href="supervised-learning.html#measuring-accuracy"><i class="fa fa-check"></i><b>5.4</b> Measuring accuracy</a><ul>
<li class="chapter" data-level="5.4.1" data-path="supervised-learning.html"><a href="supervised-learning.html#two-ways-to-be-right-true-positives-and-true-negatives"><i class="fa fa-check"></i><b>5.4.1</b> Two ways to be right: True Positives and True Negatives</a></li>
<li class="chapter" data-level="5.4.2" data-path="supervised-learning.html"><a href="supervised-learning.html#two-ways-to-be-wrong-false-postive-and-false-negatives"><i class="fa fa-check"></i><b>5.4.2</b> Two ways to be wrong: False Postive and False Negatives</a></li>
<li class="chapter" data-level="5.4.3" data-path="supervised-learning.html"><a href="supervised-learning.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>5.4.3</b> Sensitivity and Specificity</a></li>
<li class="chapter" data-level="5.4.4" data-path="supervised-learning.html"><a href="supervised-learning.html#other-measures-of-accuracy"><i class="fa fa-check"></i><b>5.4.4</b> Other measures of accuracy</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="supervised-learning.html"><a href="supervised-learning.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>5.5</b> <span class="math inline">\(k\)</span>-Nearest Neighbours</a><ul>
<li class="chapter" data-level="5.5.1" data-path="supervised-learning.html"><a href="supervised-learning.html#training-and-evaluating-knn"><i class="fa fa-check"></i><b>5.5.1</b> Training and evaluating <span class="math inline">\(k\)</span>NN</a></li>
<li class="chapter" data-level="5.5.2" data-path="supervised-learning.html"><a href="supervised-learning.html#using-a-trained-model"><i class="fa fa-check"></i><b>5.5.2</b> Using a trained model</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forest"><i class="fa fa-check"></i><b>5.6</b> Random Forest</a><ul>
<li class="chapter" data-level="5.6.1" data-path="supervised-learning.html"><a href="supervised-learning.html#building-a-random-forest-model"><i class="fa fa-check"></i><b>5.6.1</b> Building a Random Forest Model</a></li>
<li class="chapter" data-level="5.6.2" data-path="supervised-learning.html"><a href="supervised-learning.html#testing-a-random-forest-model"><i class="fa fa-check"></i><b>5.6.2</b> Testing a Random Forest model</a></li>
<li class="chapter" data-level="5.6.3" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forest-with-categorical-predictors"><i class="fa fa-check"></i><b>5.6.3</b> Random Forest with categorical predictors</a></li>
<li class="chapter" data-level="5.6.4" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forest-regression"><i class="fa fa-check"></i><b>5.6.4</b> Random Forest Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>6</b> Deep Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="deep-learning.html"><a href="deep-learning.html#about-this-chapter-3"><i class="fa fa-check"></i><b>6.1</b> About this chapter</a></li>
<li class="chapter" data-level="6.2" data-path="deep-learning.html"><a href="deep-learning.html#feature-selection-in-deep-learning"><i class="fa fa-check"></i><b>6.2</b> Feature Selection in Deep Learning</a></li>
<li class="chapter" data-level="6.3" data-path="deep-learning.html"><a href="deep-learning.html#cryptic-patterns-in-deep-learning"><i class="fa fa-check"></i><b>6.3</b> Cryptic patterns in Deep Learning</a><ul>
<li class="chapter" data-level="6.3.1" data-path="deep-learning.html"><a href="deep-learning.html#implications-of-cryptic-patterns"><i class="fa fa-check"></i><b>6.3.1</b> Implications of Cryptic Patterns</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="deep-learning.html"><a href="deep-learning.html#neural-networks"><i class="fa fa-check"></i><b>6.4</b> Neural Networks</a><ul>
<li class="chapter" data-level="6.4.1" data-path="deep-learning.html"><a href="deep-learning.html#the-perceptron"><i class="fa fa-check"></i><b>6.4.1</b> The Perceptron</a></li>
<li class="chapter" data-level="6.4.2" data-path="deep-learning.html"><a href="deep-learning.html#the-network"><i class="fa fa-check"></i><b>6.4.2</b> The Network</a></li>
<li class="chapter" data-level="6.4.3" data-path="deep-learning.html"><a href="deep-learning.html#neural-network-structure"><i class="fa fa-check"></i><b>6.4.3</b> Neural Network Structure</a></li>
<li class="chapter" data-level="6.4.4" data-path="deep-learning.html"><a href="deep-learning.html#training-to-find-weights"><i class="fa fa-check"></i><b>6.4.4</b> Training to find weights</a></li>
<li class="chapter" data-level="6.4.5" data-path="deep-learning.html"><a href="deep-learning.html#neural-network-training-phases-are-long-and-involved"><i class="fa fa-check"></i><b>6.4.5</b> Neural network training phases are long and involved</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="deep-learning.html"><a href="deep-learning.html#a-simple-neural-network-in-r"><i class="fa fa-check"></i><b>6.5</b> A simple neural network in R</a><ul>
<li class="chapter" data-level="6.5.1" data-path="deep-learning.html"><a href="deep-learning.html#frog-data"><i class="fa fa-check"></i><b>6.5.1</b> Frog Data</a></li>
<li class="chapter" data-level="6.5.2" data-path="deep-learning.html"><a href="deep-learning.html#training-a-3-hidden-layer-neural-network"><i class="fa fa-check"></i><b>6.5.2</b> Training a 3 hidden layer neural network</a></li>
<li class="chapter" data-level="6.5.3" data-path="deep-learning.html"><a href="deep-learning.html#testing-the-neural-network"><i class="fa fa-check"></i><b>6.5.3</b> Testing the neural network</a></li>
<li class="chapter" data-level="6.5.4" data-path="deep-learning.html"><a href="deep-learning.html#examining-the-structure-of-the-neural-network"><i class="fa fa-check"></i><b>6.5.4</b> Examining the structure of the neural network</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Tour of Machine Learning Tools</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="unsupervised-learning" class="section level1">
<h1><span class="header-section-number">Topic 4</span> Unsupervised Learning</h1>
<div id="about-this-chapter-1" class="section level2">
<h2><span class="header-section-number">4.1</span> About this chapter</h2>
<ol style="list-style-type: decimal">
<li>Questions
<ul>
<li>How can I find groups of similar things in data?</li>
</ul></li>
<li>Objectives
<ul>
<li>Understand features and cases</li>
<li>Understand hierarchical and k-means clustering</li>
</ul></li>
<li>Keypoints
<ul>
<li>Unsupervised learning is finding groups in a data set without known examples</li>
<li>The number of cases we have should be greater than the number of features each case has</li>
</ul></li>
</ol>
<p>In this chapter we’ll take a look at unsupervised learning tools. This is a great place to start with ML as a biologist because, whether you know it or not, you’re actually already familiar with a good number of the principles in this field. Unsupervised learning is a form of data-driven ML. In these approaches we start off with a mish-mash of things that we have information about but we don’t know what any of them are. Are aim is to try and group the similar things together, and the different things apart.</p>
</div>
<div id="p-features-and-n-cases" class="section level2">
<h2><span class="header-section-number">4.2</span> <span class="math inline">\(p\)</span> Features and <span class="math inline">\(n\)</span> Cases</h2>
<p>When we talk about information that we know about things, we really mean the things we noted in the experiment. This ‘information’ can be diverse things including but not limited to a biological sequence, a set of physical measurements, some category values, or gene expression values. There are different types of ML tool for dealing with them all. In a very general sense, what ML tools work with is an <span class="math inline">\(n \times p\)</span> matrix of <span class="math inline">\(n\)</span> cases and <span class="math inline">\(p\)</span> features, the features <span class="math inline">\(p\)</span> are the things we change and the cases <span class="math inline">\(n\)</span> are the different items or individuals we measured the features on, here’s a basic example in which the presence of a feature is indicated by a 1 and it’s absence by 0.</p>
<p><img src="figs/npmat.png" /><!-- --></p>
<p>Here’s a gene expression based example, gene counts from an RNAseq experiment. The treatments are <span class="math inline">\(p\)</span>, the different genes are the <span class="math inline">\(n\)</span>s</p>
<table>
<thead>
<tr class="header">
<th align="left">geneid</th>
<th align="right">trt1</th>
<th align="right">trt2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">gene1</td>
<td align="right">9.313663</td>
<td align="right">5.683347</td>
</tr>
<tr class="even">
<td align="left">gene2</td>
<td align="right">16.824577</td>
<td align="right">12.921582</td>
</tr>
<tr class="odd">
<td align="left">gene3</td>
<td align="right">11.134654</td>
<td align="right">18.386286</td>
</tr>
<tr class="even">
<td align="left">gene4</td>
<td align="right">18.245261</td>
<td align="right">13.271525</td>
</tr>
<tr class="odd">
<td align="left">gene5</td>
<td align="right">19.107009</td>
<td align="right">11.849221</td>
</tr>
</tbody>
</table>
<p>Finally, here’s a sequence based example</p>
<table>
<thead>
<tr class="header">
<th align="left">id</th>
<th align="left">seq</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">gene1</td>
<td align="left">TESVI</td>
</tr>
<tr class="even">
<td align="left">gene2</td>
<td align="left">NESCI</td>
</tr>
<tr class="odd">
<td align="left">gene3</td>
<td align="left">TESNI</td>
</tr>
<tr class="even">
<td align="left">gene4</td>
<td align="left">LEDVT</td>
</tr>
<tr class="odd">
<td align="left">gene5</td>
<td align="left">ANDVI</td>
</tr>
</tbody>
</table>
<p>In general ML tools need this to be true</p>
<p><span class="math inline">\(n &gt;&gt; p\)</span></p>
<p><span class="math inline">\(n\)</span> must be much greater than <span class="math inline">\(p\)</span>. We must have many more cases than features we measured. Most ML tools will fail, or at least have reduced power when this isn’t true. This can be a limiting factor in our ability to use ML. Conversely, the power will generally go up when it is true and very large data sets can give extraordinary ML power.</p>
</div>
<div id="clustering" class="section level2">
<h2><span class="header-section-number">4.3</span> Clustering</h2>
<p>The first class of ML tools we will look it is unsupervised clustering, this will be familiar to many biologists from heatmaps of gene expression data, but also more fundamentally from phylogenetic tree analysis. We’ll look at a general overview before we look at some specific tools.</p>
<div id="the-distance-measure-and-matrix" class="section level3">
<h3><span class="header-section-number">4.3.1</span> The distance measure and matrix</h3>
<p>The first step of clustering is to get a measure of ‘distance’ between all the pairs of cases <span class="math inline">\(n\)</span>s that we have gathered. In this case when we say ‘distance’, we mean a numeric measure of how similar or dissimilar our cases are. There are lots of different metrics of distance, e.g the correlation coefficient <span class="math inline">\(r\)</span> is a measure of how similar two sets of numbers are. With this measure the higher the value, the more similar the cases are. Different types of data will need different distance metrics. For sequence based data we typically have the substitution or edit distance (the number of changes needed to make the two sequences identical).</p>
<p>The distance measure is a crucial step in clustering, but all tools have a sensible default and we don’t need to worry about what it is <em>exactly</em> at this stage beyond what we’ve discussed, but we do need to see what we do with the pairs of distances to understand the basics of the algorithm. Once we have the distances we form a distance matrix, which is always square, symmetrical across the diagonal and looks like this:</p>
<pre><code>##       gene1 gene2 gene3 gene4 gene5
## gene1  0.00  1.00  0.87  1.00  0.96
## gene2  1.00  0.00  0.77  1.00  0.99
## gene3  0.87  0.77  0.00  0.97  1.00
## gene4  1.00  1.00  0.97  0.00  0.99
## gene5  0.96  0.99  1.00  0.99  0.00</code></pre>
<p>As we can see, the further apart the genes expression across the treatments the greater the distance measure. Once we have this matrix the clustering can begin.</p>
</div>
</div>
<div id="hierarchical-single-linkage-clustering" class="section level2">
<h2><span class="header-section-number">4.4</span> Hierarchical (single linkage) Clustering</h2>
<p>Hierarchical clustering is the most common and straightforward clustering algorithm. The elements (cases or <span class="math inline">\(n\)</span>s) are formed into the distance matrix and the aim is to group the pair of elements with the smallest distance into one, then repeat, continuing until we run out of elements. We then move onto grouping the pairs and so on until there’s nothing left to group. Allison Horst demonstrates it better than I, so here’s the excellent illustrations she made that run through the process.</p>
<div class="figure"><span id="fig:unnamed-chunk-23-1"></span>
<img src="figs/cluster_single_linkage_1.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.1: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-23-2"></span>
<img src="figs/cluster_single_linkage_2.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.2: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-23-3"></span>
<img src="figs/cluster_single_linkage_3.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.3: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-23-4"></span>
<img src="figs/cluster_single_linkage_4.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.4: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-23-5"></span>
<img src="figs/cluster_single_linkage_5.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.5: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-23-6"></span>
<img src="figs/cluster_single_linkage_6.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.6: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-23-7"></span>
<img src="figs/cluster_single_linkage_7.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.7: Artwork by @allison_horst
</p>
</div>
<p>Hopefully this montage has clarified the overall process of grouping elements based on distance metrics calculated between all pairs. The question remains though, how can we do this in R?</p>
<div id="hierarchical-clustering-in-base-r" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Hierarchical clustering in Base R</h3>
<p>There is an <code>hclust()</code> function built into R we can use. Being part of the base distribution and not using any packages means that this function is a bit general and needs data in particular format. Specifically it needs a numbers only matrix or data frame of information - you’d need to remove all text information from the object - getting this in to shape is left as an exercise for the reader. You would end up with a matrix object looking something like this</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="unsupervised-learning.html#cb27-1"></a>data_mat</span></code></pre></div>
<pre><code>##            trt1      trt2
## gene1  9.313663  5.683347
## gene2 16.824577 12.921582
## gene3 11.134654 18.386286
## gene4 18.245261 13.271525
## gene5 19.107009 11.849221</code></pre>
<p>You can the get clusters by creating the <code>dist</code> object with the <code>dist</code> function and the clusters with <code>hclust()</code> using the method <code>single</code> to apply the single linkage clustering we learned above. Then we can directly plot the dendrogram.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="unsupervised-learning.html#cb29-1"></a>data_dist &lt;-<span class="st"> </span><span class="kw">dist</span>(data_mat, <span class="dt">diag=</span><span class="ot">TRUE</span>)</span>
<span id="cb29-2"><a href="unsupervised-learning.html#cb29-2"></a>clusters &lt;-<span class="st"> </span><span class="kw">hclust</span>(data_dist, <span class="dt">method=</span><span class="st">&quot;single&quot;</span>)</span>
<span id="cb29-3"><a href="unsupervised-learning.html#cb29-3"></a><span class="kw">plot</span>(clusters)</span></code></pre></div>
<p><img src="machine_learning_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Note that the distance measure by default is <code>euclidean</code> which is a different way of computing distances than the <span class="math inline">\(r\)</span> correlation coefficient we discussed earlier. <code>euclidean</code> is more commonly used, but it’s beyond the scope of this course to discuss distance measures in detail. More information on distance measures is freely available on <a href="https://en.wikipedia.org/wiki/Distance">Wikipedia</a>.</p>
</div>
<div id="clustered-heatmaps" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Clustered Heatmaps</h3>
<p>Typically you’ll want to make some sort of heatmap and have a tree or dendrogram of the clusters stuck on the side, rather than just have a cluster tree on its own. Again, base R has a helpful if general function, <code>heatmap()</code>, simply pass the matrix object e.g <code>data_mat</code> and it can do the rest.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="unsupervised-learning.html#cb30-1"></a><span class="kw">heatmap</span>(data_mat)</span></code></pre></div>
<p><img src="machine_learning_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>The function has a lot of customisation options, which you can investigate using <code>?heatmap</code> and Google!</p>
</div>
<div id="extra-credit-ggplot-and-clusters" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Extra Credit: ggplot and clusters</h3>
<p>Heatmaps can be drawn in <code>ggplot</code> using the <code>geom_tile()</code> geom. If our data are in ‘tidy’ format like this</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="unsupervised-learning.html#cb31-1"></a>tidy_gdf</span></code></pre></div>
<pre><code>## # A tibble: 10 x 3
##    geneid treatment expression
##    &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;
##  1 gene1  trt1            9.31
##  2 gene1  trt2            5.68
##  3 gene2  trt1           16.8 
##  4 gene2  trt2           12.9 
##  5 gene3  trt1           11.1 
##  6 gene3  trt2           18.4 
##  7 gene4  trt1           18.2 
##  8 gene4  trt2           13.3 
##  9 gene5  trt1           19.1 
## 10 gene5  trt2           11.8</code></pre>
<p>We can make a heatmap quite simply, like this</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="unsupervised-learning.html#cb33-1"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb33-2"><a href="unsupervised-learning.html#cb33-2"></a>hmap &lt;-<span class="st"> </span><span class="kw">ggplot</span>(tidy_gdf) <span class="op">+</span><span class="st"> </span><span class="kw">aes</span>(treatment, geneid) <span class="op">+</span><span class="st"> </span><span class="kw">geom_tile</span>(<span class="kw">aes</span>(<span class="dt">fill=</span>expression))</span>
<span id="cb33-3"><a href="unsupervised-learning.html#cb33-3"></a>hmap</span></code></pre></div>
<p><img src="machine_learning_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>But this has no dendrogram and is not clustered! It’s going to take a little fiddling to add this on - we can get the clusters out of the <code>hclust()</code> result and apply those. To do that we must solve another problem first - turning our tidy data into a matrix! That can be done with <code>pivot_wider()</code> from <code>tidyr</code>, which gets us most of the way there.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="unsupervised-learning.html#cb34-1"></a><span class="kw">library</span>(tidyr)</span>
<span id="cb34-2"><a href="unsupervised-learning.html#cb34-2"></a>wide_gdf &lt;-<span class="st"> </span>tidy_gdf <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pivot_wider</span>(</span>
<span id="cb34-3"><a href="unsupervised-learning.html#cb34-3"></a>  <span class="dt">id_cols =</span> <span class="st">&quot;geneid&quot;</span>, </span>
<span id="cb34-4"><a href="unsupervised-learning.html#cb34-4"></a>  <span class="dt">names_from=</span><span class="st">&quot;treatment&quot;</span>, </span>
<span id="cb34-5"><a href="unsupervised-learning.html#cb34-5"></a>  <span class="dt">values_from=</span><span class="st">&quot;expression&quot;</span>)</span>
<span id="cb34-6"><a href="unsupervised-learning.html#cb34-6"></a>wide_gdf</span></code></pre></div>
<pre><code>## # A tibble: 5 x 3
##   geneid  trt1  trt2
##   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 gene1   9.31  5.68
## 2 gene2  16.8  12.9 
## 3 gene3  11.1  18.4 
## 4 gene4  18.2  13.3 
## 5 gene5  19.1  11.8</code></pre>
<p>And we can now remove the non-numeric columns and do the cluster</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="unsupervised-learning.html#cb36-1"></a>data_mat &lt;-<span class="st"> </span>wide_gdf <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb36-2"><a href="unsupervised-learning.html#cb36-2"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>geneid) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb36-3"><a href="unsupervised-learning.html#cb36-3"></a><span class="st">  </span><span class="kw">as.matrix</span>()</span>
<span id="cb36-4"><a href="unsupervised-learning.html#cb36-4"></a></span>
<span id="cb36-5"><a href="unsupervised-learning.html#cb36-5"></a>clusters &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(data_mat, <span class="dt">diag=</span><span class="ot">TRUE</span>))</span></code></pre></div>
<p>We can reorder the axis in our <code>ggplot</code> heatmap using the <code>order</code> from the <code>clusters</code> object to put the <code>geneid</code>s into the right order</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="unsupervised-learning.html#cb37-1"></a>clusters<span class="op">$</span>order</span></code></pre></div>
<pre><code>## [1] 1 3 5 2 4</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="unsupervised-learning.html#cb39-1"></a>hmap <span class="op">+</span><span class="st"> </span><span class="kw">scale_y_discrete</span>(<span class="dt">limits=</span> wide_gdf<span class="op">$</span>geneid[clusters<span class="op">$</span>order] )</span></code></pre></div>
<p><img src="machine_learning_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>The <code>ggdendro</code> package allows us to create a dendrogram from a clustering</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="unsupervised-learning.html#cb40-1"></a><span class="kw">library</span>(ggdendro)</span>
<span id="cb40-2"><a href="unsupervised-learning.html#cb40-2"></a>dendro &lt;-<span class="st"> </span><span class="kw">ggdendrogram</span>(clusters) <span class="op">+</span><span class="st"> </span><span class="kw">coord_flip</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme_dendro</span>()</span>
<span id="cb40-3"><a href="unsupervised-learning.html#cb40-3"></a>dendro</span></code></pre></div>
<p><img src="machine_learning_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>We can compose the two plots with <code>patchwork</code> (cheekily moving the legend out of the way first).</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="unsupervised-learning.html#cb41-1"></a><span class="kw">library</span>(patchwork)</span>
<span id="cb41-2"><a href="unsupervised-learning.html#cb41-2"></a>hmap <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&quot;bottom&quot;</span>) <span class="op">+</span><span class="st">  </span>dendro</span></code></pre></div>
<p><img src="machine_learning_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
</div>
</div>
<div id="k-means-clustering" class="section level2">
<h2><span class="header-section-number">4.5</span> K-Means clustering</h2>
<p>A limitation of hierarchical clustering is that we as the operator have to guess what elements are in which cluster and that can be a bit arbitrary. An alternative algorithm, the K-means cluster gets around this problem by allowing us to specify the number of clusters up-front and works from there. It starts with the assumption that there are <code>k</code> clusters and makes <code>k</code> random cluster start points (centroids) then tries to assign cases (elements/observations) to one of each centroid based on the distance from the start points. The assignment to clusters is improved iteratively by starting again with the centroid at the mean point in each cluster and continues until no improvements are made. Again, Allison Horst has drawn some great guides</p>
<div class="figure"><span id="fig:unnamed-chunk-36-1"></span>
<img src="figs/kmeans_1.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.8: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-36-2"></span>
<img src="figs/kmeans_2.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.9: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-36-3"></span>
<img src="figs/kmeans_3.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.10: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-36-4"></span>
<img src="figs/kmeans_4.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.11: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-36-5"></span>
<img src="figs/kmeans_5.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.12: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-36-6"></span>
<img src="figs/kmeans_6.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.13: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-36-7"></span>
<img src="figs/kmeans_7.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.14: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-36-8"></span>
<img src="figs/kmeans_8.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.15: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-36-9"></span>
<img src="figs/kmeans_9.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.16: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-36-10"></span>
<img src="figs/kmeans_10.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.17: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-36-11"></span>
<img src="figs/kmeans_11.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.18: Artwork by @allison_horst
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-36-12"></span>
<img src="figs/kmeans_12.jpg" alt="Artwork by \@allison_horst"  />
<p class="caption">
Figure 4.19: Artwork by @allison_horst
</p>
</div>
<div id="figure-of-merit" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Figure of Merit</h3>
<p>A limitation of this approach and of hierarchical clustering is that we may not know how many <code>k</code> clusters there are. The Figure of Merit (FOM) technique can help us work out the <code>k</code> that we need. Briefly, this works by trying a k-means clustering at 1, then 2, then 3 up to a stopping number of clusters and at the end of each clustering we check the distance variability between the centroids and the elements/cases/observations. The value of <code>k</code> that minimises the distance is the value that we want to use as most points are near to a cluster centroid.</p>
<p>Let’s walk through the process of doing FOM and then applying a k-means clustering.</p>
<p>Here’s a sample data set to try and cluster, we’re going to cluster the rows. Note how it resembles a gene expression matrix with the gene names as the matrix row names, not in the data itself.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="unsupervised-learning.html#cb42-1"></a><span class="kw">head</span>(gene_exprs, <span class="dt">n=</span><span class="dv">3</span>)</span></code></pre></div>
<pre><code>##      sample1  sample2  sample3
## gn1 4.516471 3.722275 15.52375
## gn2 1.451247 3.225752 15.21140
## gn3 3.584614 3.020616 15.40538</code></pre>
<p>We’ll use the <code>factoextra</code> package to do the FOM and the subsequent k-means. First the FOM using <code>fviz_nbclust()</code></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="unsupervised-learning.html#cb44-1"></a><span class="kw">library</span>(factoextra)</span>
<span id="cb44-2"><a href="unsupervised-learning.html#cb44-2"></a><span class="kw">fviz_nbclust</span>(gene_exprs, kmeans, <span class="dt">method=</span><span class="st">&quot;wss&quot;</span>)</span></code></pre></div>
<p><img src="machine_learning_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>The option <code>method</code> lets us specify which method we want to use to estimate the variability, here we use <code>wss</code> for within sum of squares, which is a reasonable one. The resulting plot shows that <code>wss</code> improves lots until we get to 3 clusters, at which point there is only minimal improvement. We interpret this as meaning that there are 3 clusters within our data. We can use that to make our k-means cluster. The <code>kmeans()</code> function does this easily and we can plot the result using the <code>fviz_cluster()</code> function.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="unsupervised-learning.html#cb45-1"></a>km_clus &lt;-<span class="st"> </span><span class="kw">kmeans</span>(gene_exprs, <span class="dv">3</span>, <span class="dt">nstart=</span><span class="dv">25</span>, <span class="dt">iter.max =</span> <span class="dv">1000</span>)</span>
<span id="cb45-2"><a href="unsupervised-learning.html#cb45-2"></a><span class="kw">fviz_cluster</span>(km_clus, <span class="dt">data=</span>gene_exprs)</span></code></pre></div>
<p><img src="machine_learning_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<p>The plot shows clearly the elements of the data are clustered into 3 groups. The <code>km_clus</code> object contains information about the elements cluster membership if you wish to extract that for any reason.</p>

<div class="roundup">
<ul>
<li>Unsupervised learning algorithms group things based on distances computed between them.</li>
<li>Hierarchical and k-means are two common and useful methods.</li>
</ul>
</div>


</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="r-fundamentals.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="supervised-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-unsupervised.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machine_learning.pdf", "machine_learning.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
