<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.387">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>A Tour of Machine Learning Tools - 3&nbsp; Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<link href="./prerequisites.html" rel="next">
<link href="./04-supervised.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link id="quarto-text-highlighting-styles" href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">A Tour of Machine Learning Tools</a> 
        <div class="sidebar-tools-main">
    <a href="./A-Tour-of-Machine-Learning-Tools.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
    <a href="" title="Share" id="sidebar-tool-dropdown-0" class="sidebar-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi bi-share"></i></a>
    <ul class="dropdown-menu" aria-labelledby="sidebar-tool-dropdown-0">
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
            <i class="bi bi-bi-twitter pe-1"></i>
          Twitter
          </a>
        </li>
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
            <i class="bi bi-bi-facebook pe-1"></i>
          Facebook
          </a>
        </li>
    </ul>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Motivation</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-unsupervised.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-supervised.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Supervised Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-deep_learning.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Appendices</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prerequisites.html" class="sidebar-item-text sidebar-link">Prerequisites</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./r-fundamentals.html" class="sidebar-item-text sidebar-link">R Fundamentals</a>
  </div>
</li>
    </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#about-this-chapter" id="toc-about-this-chapter" class="nav-link active" data-scroll-target="#about-this-chapter"> <span class="header-section-number">3.1</span> About this chapter</a></li>
  <li><a href="#feature-selection-in-deep-learning" id="toc-feature-selection-in-deep-learning" class="nav-link" data-scroll-target="#feature-selection-in-deep-learning"> <span class="header-section-number">3.2</span> Feature Selection in Deep Learning</a></li>
  <li><a href="#cryptic-patterns-in-deep-learning" id="toc-cryptic-patterns-in-deep-learning" class="nav-link" data-scroll-target="#cryptic-patterns-in-deep-learning"> <span class="header-section-number">3.3</span> Cryptic patterns in Deep Learning</a>
  <ul class="collapse">
  <li><a href="#implications-of-cryptic-patterns" id="toc-implications-of-cryptic-patterns" class="nav-link" data-scroll-target="#implications-of-cryptic-patterns"> <span class="header-section-number">3.3.1</span> Implications of Cryptic Patterns</a></li>
  </ul></li>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link" data-scroll-target="#neural-networks"> <span class="header-section-number">3.4</span> Neural Networks</a>
  <ul class="collapse">
  <li><a href="#the-perceptron" id="toc-the-perceptron" class="nav-link" data-scroll-target="#the-perceptron"> <span class="header-section-number">3.4.1</span> The Perceptron</a></li>
  <li><a href="#the-network" id="toc-the-network" class="nav-link" data-scroll-target="#the-network"> <span class="header-section-number">3.4.2</span> The Network</a></li>
  <li><a href="#neural-network-structure" id="toc-neural-network-structure" class="nav-link" data-scroll-target="#neural-network-structure"> <span class="header-section-number">3.4.3</span> Neural Network Structure</a></li>
  <li><a href="#training-to-find-weights" id="toc-training-to-find-weights" class="nav-link" data-scroll-target="#training-to-find-weights"> <span class="header-section-number">3.4.4</span> Training to find weights</a></li>
  <li><a href="#neural-network-training-phases-are-long-and-involved" id="toc-neural-network-training-phases-are-long-and-involved" class="nav-link" data-scroll-target="#neural-network-training-phases-are-long-and-involved"> <span class="header-section-number">3.4.5</span> Neural network training phases are long and involved</a></li>
  </ul></li>
  <li><a href="#a-simple-neural-network-in-r" id="toc-a-simple-neural-network-in-r" class="nav-link" data-scroll-target="#a-simple-neural-network-in-r"> <span class="header-section-number">3.5</span> A simple neural network in R</a>
  <ul class="collapse">
  <li><a href="#frog-data" id="toc-frog-data" class="nav-link" data-scroll-target="#frog-data"> <span class="header-section-number">3.5.1</span> Frog Data</a></li>
  <li><a href="#training-a-3-hidden-layer-neural-network" id="toc-training-a-3-hidden-layer-neural-network" class="nav-link" data-scroll-target="#training-a-3-hidden-layer-neural-network"> <span class="header-section-number">3.5.2</span> Training a 3 hidden layer neural network</a></li>
  <li><a href="#testing-the-neural-network" id="toc-testing-the-neural-network" class="nav-link" data-scroll-target="#testing-the-neural-network"> <span class="header-section-number">3.5.3</span> Testing the neural network</a></li>
  <li><a href="#examining-the-structure-of-the-neural-network" id="toc-examining-the-structure-of-the-neural-network" class="nav-link" data-scroll-target="#examining-the-structure-of-the-neural-network"> <span class="header-section-number">3.5.4</span> Examining the structure of the neural network</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"> <span class="header-section-number">3.6</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning</span></h1>
</div>





<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<ol type="1">
<li>Questions
<ul>
<li>What is Deep Learning?</li>
<li>How is Deep Learning distinct from classical Machine Learning?</li>
</ul></li>
<li>Objectives
<ul>
<li>Discuss how cryptic patterns can find their own important features in arbitrary data sets</li>
<li>Study the outline of training a Neural Network</li>
<li>Build and test a simple Neural Network</li>
</ul></li>
<li>Key Points
<ul>
<li>Deep Learners find important features and patterns in the data automatically</li>
<li>Neural Networks use optimised weights to learn classifications</li>
<li>Power comes at the expense of interpretability</li>
</ul></li>
</ol>
<section id="about-this-chapter" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="about-this-chapter"><span class="header-section-number">3.1</span> About this chapter</h2>
<p>In this chapter we’ll look at the latest advance in Machine Learning, a special set of extremely powerful techniques called Deep Learning. Deep learning methods are the ones that have been used in headline grabbing Artificial Intelligence toold from large scale facial recognition, data mining to influence voters on social media, voice recognition like Siri and Alexa, Netflix’s suggestion algorithm, Google’s advertising algorithm, and in science things like AlphaFold - the protein folding prediction tool and medical image analysis.</p>
<p>These tools and algorithms are a wide family of their own and have properties distinct from the machine learning techniques we’ve looked at so far, principally these types are able to select the most important features themselves and work out what is the most reliable data. They are also much more complicated in practice and much more dense so they become a black box and we are less able to interpret how they are making the decisions they make. This is the trade off we make when using Deep Learning.</p>
</section>
<section id="feature-selection-in-deep-learning" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="feature-selection-in-deep-learning"><span class="header-section-number">3.2</span> Feature Selection in Deep Learning</h2>
<p>In the previous tools we’ve looked at we used a <span class="math inline">\(np\)</span> feature matrix, with <span class="math inline">\(p\)</span> features - a column of data for each thing we measured. Feature selection is really important and can make or break the usefulness of a machine learning tool. If the features we select can’t differentiate between the classes, then the machine learning tool will never be able to make good predictions. Consider what it would be like if we tried to work out a person’s hair colour from their height! Height is an easy thing to measure but does it ever predict a person’s hair?</p>
<p>So we must pick our features carefully if we’re to make use of ML generally, but with Deep Learning the algorithms themselves work out which are the most useful features and also patterns within the features and preferentially use them. This leads to a bit of a kitchen sink approach, we can take all the features we like, pump them into a Deep Learning algorithm and let it decide the best way to use them. A practical upside of this then is that our <span class="math inline">\(np\)</span> feature matrix can become very complicated and we can start to squeeze pretty much anything into the training data. We simply have to be able to encode it as numbers somehow.</p>
</section>
<section id="cryptic-patterns-in-deep-learning" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="cryptic-patterns-in-deep-learning"><span class="header-section-number">3.3</span> Cryptic patterns in Deep Learning</h2>
<p>The ability to automatically select features or patterns to use means that the algorithms can find and use patterns that we don’t specify explicitly and in fact don’t even know about. To understand this, let’s work through a protein sequence based example. Our first issue with biological sequences is the question of how to encode it as numbers. One common way of taking a categoric thing and making it numeric is to use ‘One Hot Encoding’. Which looks like this:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="figs/onehot.png" class="img-fluid" width="357"></p>
</div>
</div>
<p>This encoding represents the protein sequence ‘ACDE’, the columns represent the alphabet of amino acids (in alphabetic order), the rows represent the position of sequence. We add a <code>1</code> at the intersection of the position and amino acid to show the amino acid at each position. Each row therefore has only one row.</p>
<p>Once we have an encoding, patterns will start to appear that the algorithms can use. Consider a protein motif, like RHLR - that would look like this in our one hot encoding.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="figs/rhlr.png" class="img-fluid" width="356"></p>
</div>
</div>
<p>Now wherever the pattern crops up the algorithm will see it. The pattern can become associated with a particular class and used as part of the signal for classification. We didn’t have to say ‘protein has RHLR’, if it’s an important pattern and associated with a class or group then the algorithm will use it.</p>
<section id="implications-of-cryptic-patterns" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="implications-of-cryptic-patterns"><span class="header-section-number">3.3.1</span> Implications of Cryptic Patterns</h3>
<p>Being able to select its own features and patterns means that the Deep Learning methods get a special sort of sensitivity. Considering our protein example again, then lots of properties of the proteins that are reliant on sequence at some level will be detectable and useable in some way by the Deep Learner. Things like physico-chemical sequence properties such as hydrophobicity are reliant on the actual amino acids to exist so they can be captured and used.</p>
<p>An important thing to note is that the patterns have only to be associated, not <em>over-represented</em>, on the whole. The Deep Learners might find a pattern that occurs only a few times in millions of example data, but if it is associated pretty uniquely with just one class or group then it can be used. This stands in contrast to typical methods of pattern finding in bioinformatics, which use majority or statistical over-representation. The patterns often have weight with other patterns and these associations increase the patterns power too.</p>
<p>The ability to find cryptic patterns and make associations is reliant on having a great deal of training data. Deep Learning methods do require lots more data than the ML methods we’ve already looked at and this can be a drawback in practice.</p>
<p>Deep Learning models internal representations become very large and hard to interpret, so that actually understanding what they’re using to classify upon can become impossible. This is a significant trade off for the high power that we can get</p>
</section>
</section>
<section id="neural-networks" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="neural-networks"><span class="header-section-number">3.4</span> Neural Networks</h2>
<p>The core of most Deep Learning models and model types is the neural network, let’s run through how that works to gain some insight into how it gets its power.</p>
<section id="the-perceptron" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="the-perceptron"><span class="header-section-number">3.4.1</span> The Perceptron</h3>
<p>Neural networks are made up of units called Perceptrons, these are mathematical structures inspired by biological neurons. They take multiple inputs, integrate them in some way and produce an output. One that worked on our animal matrix might look like this</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="figs/percepwts/Slide1.png" class="img-fluid" width="360"></p>
</div>
</div>
</section>
<section id="the-network" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="the-network"><span class="header-section-number">3.4.2</span> The Network</h3>
<p>Combining lots of perceptrons results in a neural network and at a basic level might work with our animal data like this,</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="figs/percepwts/Slide2.png" class="img-fluid" width="360"></p>
</div>
</div>
<p>Making sense of all the integrations from the neural network, that is the calling of a class (in this case <code>is_a_cat</code>) is done by a decision function, here that may look like this</p>
<pre><code>if S &gt; 2,
    cat = 1
else 
    cat = 0</code></pre>
<p>Applied to the animal neural network the animal data classifications end up like this</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="figs/percepwts/Slide3.png" class="img-fluid" width="360"></p>
</div>
</div>
<p>Only one of the actual cats was correctly labelled. The clever part of the neural network is to apply weights to each of the features that modify the value they add to the neural network (in the figure below as blue values).</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="figs/percepwts/Slide4.png" class="img-fluid" width="360"></p>
</div>
</div>
<p>Weights work to give the more useful features higher values (like <code>meows</code>) and less useful features lower values (like <code>four legs</code>). The network can now more accurately classify the animals in the picture.</p>
</section>
<section id="neural-network-structure" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="neural-network-structure"><span class="header-section-number">3.4.3</span> Neural Network Structure</h3>
<p>The networks needn’t be restricted to simple structures in which the initial inputs go straight to the output, many layers of neurons can be made, each arbitrary numbers of neurons deep. These extra layers are called the hidden layers</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="figs/layers.png" class="img-fluid" width="274"></p>
</div>
</div>
<p>The hidden layers increase the power of the neural network by allowing for further integration of information and extra weighting. But they also make the network more obscure and hard to read, again this is how the power of the neural network comes at the expense of interpretability.</p>
</section>
<section id="training-to-find-weights" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="training-to-find-weights"><span class="header-section-number">3.4.4</span> Training to find weights</h3>
<p>The main part of training the neural network and the place it makes itself powerful is in the weight finding phase. This is called the learning or training phase. To do this the training algorithm goes back and forth across the network methodically adjusting the weights until it sees no further improvement when classifying on the training data - it is constantly comparing its current state against the answers in the training data.</p>
</section>
<section id="neural-network-training-phases-are-long-and-involved" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="neural-network-training-phases-are-long-and-involved"><span class="header-section-number">3.4.5</span> Neural network training phases are long and involved</h3>
<p>As you can see there is a lot about neural networks to be specified and optimised. The number and depth of hidden layers that is optimal varies for each data set and there is no rule to follow as to what will be best. It is also not true that bigger is always better. The weights of the neural network must also be optimised for every data set, and we must be careful to use training data that is distinct from our test data to be confident in the generality of our resulting model. As a result of these considerations the training and testing phases of neural networks are particularly involved.</p>
<p>We won’t go through that whole procedure here, though you should be aware of it as it is the key to a truly useful deep learning model. But we will try out a small neural network in R.</p>
</section>
</section>
<section id="a-simple-neural-network-in-r" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="a-simple-neural-network-in-r"><span class="header-section-number">3.5</span> A simple neural network in R</h2>
<section id="frog-data" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="frog-data"><span class="header-section-number">3.5.1</span> Frog Data</h3>
<p>In this example we shall use some data on amphibian presence at various sites. Here’s a <code>glimpse()</code> of the <code>train_rows</code>, we also have a <code>test_rows</code></p>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">glimpse</span>(train_rows)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 94
Columns: 20
$ SR                 &lt;dbl&gt; 1000, 100, 200, 30000, 10050, 700, 50, 8000, 2500, …
$ NR                 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 1, 1, 1, …
$ TR                 &lt;dbl&gt; 1, 1, 5, 1, 1, 5, 1, 1, 1, 2, 1, 1, 1, 1, 14, 1, 14…
$ VR                 &lt;dbl&gt; 3, 2, 1, 3, 2, 2, 2, 3, 3, 0, 1, 2, 3, 2, 3, 3, 1, …
$ SUR1               &lt;dbl&gt; 2, 2, 10, 1, 1, 10, 2, 2, 10, 6, 2, 2, 2, 2, 7, 2, …
$ SUR2               &lt;dbl&gt; 1, 7, 6, 1, 10, 6, 7, 10, 2, 9, 7, 7, 2, 10, 2, 2, …
$ SUR3               &lt;dbl&gt; 9, 6, 10, 1, 6, 9, 10, 7, 6, 2, 6, 9, 1, 10, 1, 7, …
$ UR                 &lt;dbl&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, …
$ FR                 &lt;dbl&gt; 0, 0, 4, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, …
$ OR                 &lt;dbl&gt; 100, 100, 75, 100, 100, 100, 100, 100, 100, 50, 100…
$ RR                 &lt;dbl&gt; 2, 2, 1, 2, 5, 1, 5, 9, 0, 0, 0, 0, 5, 1, 5, 0, 5, …
$ BR                 &lt;dbl&gt; 5, 2, 1, 10, 5, 1, 5, 9, 1, 0, 0, 0, 5, 5, 5, 1, 5,…
$ MR                 &lt;dbl&gt; 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
$ CR                 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
$ Green_frogs        &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, …
$ Brown_frogs        &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, …
$ Common_toad        &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, …
$ Tree_frog          &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, …
$ Common_newt        &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, …
$ Great_crested_newt &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …</code></pre>
</div>
</div>
<p>These data are from <span class="citation" data-cites="frogs">(<a href="#ref-frogs" role="doc-biblioref">Blachnik, Sołtysiak, and Dąbrowska 2019</a>)</span> originally and you can see a description of the 20 columns at <a href="https://archive.ics.uci.edu/ml/datasets/Amphibians">https://archive.ics.uci.edu/ml/datasets/Amphibians</a>. Briefly, they are things like the presence and size and maintenance of reservoirs and the surrounding area, whether humans use the area, whether there is fishing. All potentially pertinent measurements. The presence of different types of amphibian species are recorded as 1 for present, and 0 for not present.</p>
<p>We could use a neural network to predict any of the species listed, but let’s work on predicting <code>Green_frogs</code>.</p>
<p>We’ll use the straightforward <code>neuralnet()</code> function in the <code>neuralnet</code> package for this. It can take an R formula specification, which as you’ll recall takes the form <code>y ~ feature_1 + feature_2 ...</code> where <code>y</code> is the thing to be predicted and <code>feature_x</code> are the features to input for prediction with. With 19 to enter, that’s lots of typing, so I’ve squashed it into a variable called <code>long_formula</code></p>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>long_formula</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Green_frogs ~ SR + NR + TR + VR + SUR1 + SUR2 + SUR3 + UR + FR + 
    OR + RR + BR + MR + CR + Green_frogs + Brown_frogs + Common_toad + 
    Tree_frog + Common_newt + Great_crested_newt</code></pre>
</div>
</div>
</section>
<section id="training-a-3-hidden-layer-neural-network" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="training-a-3-hidden-layer-neural-network"><span class="header-section-number">3.5.2</span> Training a 3 hidden layer neural network</h3>
<p>We can put the formula into the function <code>neuralnet()</code> specify the training data and the depth of the hidden layers. Here we’ll have 3, with 15, 10 and 5 neurons respectively.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(neuralnet)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>nn <span class="ot">&lt;-</span> <span class="fu">neuralnet</span>(long_formula, train_rows, <span class="at">hidden=</span><span class="fu">c</span>(<span class="dv">15</span>,<span class="dv">10</span>,<span class="dv">5</span>), <span class="at">linear.output=</span><span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>That single step builds the neural network, and trains it and gives it back to use so we can use it to make predictions with. Of course the first thing we want to make predictions on is our test set so we can evaluate the accuracy.</p>
</section>
<section id="testing-the-neural-network" class="level3" data-number="3.5.3">
<h3 data-number="3.5.3" class="anchored" data-anchor-id="testing-the-neural-network"><span class="header-section-number">3.5.3</span> Testing the neural network</h3>
<p>The <code>compute()</code> function takes a neural network model and data and creates predictions. Here we feed it our test data. However when we look at the resulting predictions (stored in the <code>net.result</code> slot in our <code>predictions</code> object) we see something odd.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">compute</span>(nn, test_rows)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(predictions<span class="sc">$</span>net.result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]
[1,] 0.998154554
[2,] 0.998147802
[3,] 0.008503821
[4,] 0.998154554
[5,] 0.996971835
[6,] 0.006267082</code></pre>
</div>
</div>
<p>The predictions are not of classes, but are actually numbers that represent the level of sureness the model has that the site has Green Frogs. This value is sometimes useful, but we need to convert it to classes to evaluate it. As the values run between 0 and 1 we can do that by simple rounding so that any prediction over 0.5 is considered a present prediction, anything below is consider an absent predictions (other algorithms and functions exist for this conversion).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>binary_predictions <span class="ot">&lt;-</span> <span class="fu">round</span>(predictions<span class="sc">$</span>net.result,<span class="at">digits=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can then put those binarised predictions into the <code>confusionMatrix()</code> function we used previously alongside the true values from the <code>test_rows</code> data (remembering to convert them to <code>factors</code> as they are not already).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="fu">factor</span>(test_rows<span class="sc">$</span>Green_frogs), <span class="fu">factor</span>(binary_predictions))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction  0  1
         0 30  4
         1 19 42
                                          
               Accuracy : 0.7579          
                 95% CI : (0.6592, 0.8399)
    No Information Rate : 0.5158          
    P-Value [Acc &gt; NIR] : 1.097e-06       
                                          
                  Kappa : 0.5201          
                                          
 Mcnemar's Test P-Value : 0.003509        
                                          
            Sensitivity : 0.6122          
            Specificity : 0.9130          
         Pos Pred Value : 0.8824          
         Neg Pred Value : 0.6885          
             Prevalence : 0.5158          
         Detection Rate : 0.3158          
   Detection Prevalence : 0.3579          
      Balanced Accuracy : 0.7626          
                                          
       'Positive' Class : 0               
                                          </code></pre>
</div>
</div>
<p>We can see the resulting network has about 60% sensitivity and 90% specificity, so missing a lot of real green frog sites.</p>
<p>As you can imagine the exact choice of the parameters can make a difference on final neural network performance. This is just one instance. In real analyses we would try out many different hidden layer and other parameter configurations and select the best performing at the the testing stage. That may then even move on to a further fine tuning stage, the development of machine learners is art as much as it is science.</p>
</section>
<section id="examining-the-structure-of-the-neural-network" class="level3" data-number="3.5.4">
<h3 data-number="3.5.4" class="anchored" data-anchor-id="examining-the-structure-of-the-neural-network"><span class="header-section-number">3.5.4</span> Examining the structure of the neural network</h3>
<p>The <code>neuralnet</code> package we used here was chosen not least because it is straightforward and fast, but also because it is possible to get a plot of the created network.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="figs/nn_viz.png" class="img-fluid" width="360"></p>
</div>
</div>
<p>Note that the first layer corresponds to the input columns in the data with one neuron each, these then feed into the 3 hidden layers we specified of 20,15 and 5 layers each and finally the one neuron layer intergrating everything to give us the final prediction on whether we have a Green Frog.</p>
<p>We can see that the neural network we made is really complex. Even with just the small number of input features and hidden layers we have the combinations of weights and their effect into the next layer is too hard to understand (even if the plot were readable). This shows us how neural network structures become black boxes, we can’t be sure which of the input variables (or combinations of which) were most important in making the classifications.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Roundup
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Deep Learners choose their own features</li>
<li>Deep Learners like neural networks can work on patterns we dont explicitly state</li>
<li>Neural network training means finding weights that give the best classifications</li>
<li>Neural networks are black boxes and hard to interpret</li>
</ul>
</div>
</div>
</section>
</section>
<section id="references" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="references"><span class="header-section-number">3.6</span> References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-frogs" class="csl-entry" role="doc-biblioentry">
Blachnik, Marcin, Marek Sołtysiak, and Dominika Dąbrowska. 2019. <span>“Predicting Presence of Amphibian Species Using Features Obtained from GIS and Satellite Images.”</span> <em>ISPRS International Journal of Geo-Information</em> 8 (3). <a href="https://doi.org/10.3390/ijgi8030123">https://doi.org/10.3390/ijgi8030123</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04-supervised.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./prerequisites.html" class="pagination-link">
        <span class="nav-page-text">Prerequisites</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>